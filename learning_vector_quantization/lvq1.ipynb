{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Vector Quantization applied to text classification\n",
    "\n",
    "Learning Vector Quantization (Kohoen 1986, cf. Kohoen 1998) is a supervised classification approach. \n",
    "LVQ is a prototype-based competitive learning network that learns prototypes\n",
    "by moving 'winning' prototypes into the direction of the respective data point (and moving 'loosing'\n",
    "prototypes in the opposite direction). The prototypes are defined in the feature space of the input data,\n",
    "which makes LVQ models more readily interpretable by domain experts than other neural network approaches.\n",
    "Martín-Valdivia, Ureña-López and García-Vega (2007) applied LVQ to textual data in two different classification tasks,\n",
    "namely text categorization and word sense disambiguation, and showed that it yields superior performance in comparison\n",
    "to other common algorithms such as Naive Bayes, SVMs and k-NN.\n",
    "\n",
    "Following this line of research, I will report on a experiment in text classfication using the LVQ implementation\n",
    "provided by [Neurolab](https://github.com/zueve/neurolab). While LVQ exists in various improved variants, \n",
    "Ńeurolab only implements the basic version as originally proposed by Kohoen (LVQ1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - DBMail newsletter\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import neurolab as nl\n",
    "import pylab as pl\n",
    "import csv\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    target = []\n",
    "    for label in labels:\n",
    "        if label == 0.0:\n",
    "            target.append([1.0,0.0])\n",
    "        elif  label == 1.0:\n",
    "            target.append([0.0,1.0])\n",
    "    return target\n",
    "\n",
    "def decode_labels(targets):\n",
    "    labels = []\n",
    "    for target in targets:\n",
    "        if np.array_equal(target, [1.0,0.0]):\n",
    "            labels.append(0.0)\n",
    "        elif np.array_equal(target, [0.0,1.0]):\n",
    "            labels.append(1.0)\n",
    "    return labels\n",
    "\n",
    "def load_data(path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    feature_names = []\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter=',',)\n",
    "        header = reader.next()\n",
    "        feature_names = header[:-1] # header without CLASS\n",
    "        for row in reader:\n",
    "            row_values = map(float, row)\n",
    "            label = row_values.pop()\n",
    "            labels.append(label)\n",
    "            data.append(row_values)\n",
    "    return (data, encode_labels(labels), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = -14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_data, original_target, feature_names = load_data('dbworld_subjects_stemmed.csv')\n",
    "original_train_data = original_data[:TRAIN_TEST_SPLIT]\n",
    "original_train_target = original_target[:TRAIN_TEST_SPLIT]\n",
    "original_test_data = original_data[TRAIN_TEST_SPLIT:]\n",
    "original_test_target = original_target[TRAIN_TEST_SPLIT:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_labeled_data(original_data, original_target):\n",
    "    shuffled = zip(original_data, original_target)\n",
    "    shuffle(shuffled)\n",
    "    data = np.array([data for data, _ in shuffled])\n",
    "    target = np.array([target for _, target in shuffled])\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, train_target = shuffle_labeled_data(original_train_data, original_train_target)\n",
    "test_data, test_target = shuffle_labeled_data(original_test_data, original_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (N,d):  50 229\n",
      "test (N,d):  14 229\n"
     ]
    }
   ],
   "source": [
    "train_N, num_dimensions = train_data.shape\n",
    "test_N, _ = test_data.shape\n",
    "print 'train (N,d): ', train_N, num_dimensions\n",
    "print 'test (N,d): ', test_N, num_dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up LVQ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create network with 2 layers: d neurons (i.e. as man neurons as dimensions) in input layer (competitive) and 2 neurons (i.e. as many neurons as classes) in output layer (perceptrons with linear transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100; Error: 0.5;\n",
      "Epoch: 200; Error: 0.28;\n",
      "The maximum number of train epochs is reached\n"
     ]
    }
   ],
   "source": [
    "net = nl.net.newlvq(nl.tool.minmax(train_data), num_dimensions, [0.5, 0.5])\n",
    "error = net.train(train_data, train_target, epochs=200, lr=0.0001, goal=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = net.sim(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority baseline:  0.546875\n"
     ]
    }
   ],
   "source": [
    "class_counts = [0,0]\n",
    "for labels in target:\n",
    "    if np.array_equal(labels, [ 1.0,  0.0]):\n",
    "        class_counts[0] += 1\n",
    "    else:\n",
    "        class_counts[1] += 1\n",
    "print 'Majority baseline: ', np.max(class_counts) / float(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.785714285714\n"
     ]
    }
   ],
   "source": [
    "def evaluate(output, test_target):\n",
    "    right = 0\n",
    "    for i, labels in enumerate(output):\n",
    "        if np.array_equal(labels, test_target[i]):\n",
    "            right += 1\n",
    "    print 'Accuracy: ', right / float(len(test_target))\n",
    "evaluate(output, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFP\n",
      "['10th', '13th', '1st', '2nd', '31st', '3rd', '5th', '6th', 'abstract', 'academ', 'access', 'acm', 'acn', 'advanc', 'aie', 'algorithm', 'analysi', 'analyt', 'antil', 'appli', 'applic', 'area', 'asia', 'assist', 'associ', 'augment', 'australia', 'autonom', 'base', 'big', 'bournemouth', 'call', 'centr', 'certif', 'ceur', 'cfp', 'chicago', 'chile', 'china', 'citi', 'cloud', 'commerc', 'commun', 'comput', 'computation', 'confer', 'constraint', 'cryptographi', 'cse', 'csiro', 'dalian', 'danms\\xe2\\x80\\x9912', 'data', 'databas', 'ddi', 'deadlin', 'delai', 'delft', 'demo', 'deploy', 'deri', 'differenti', 'distribut', 'dmc', 'doc', 'doctor', 'dublin', 'due', 'edbt2012', 'effici', 'electron', 'energi', 'environ', 'er2012', 'event', 'exhibit', 'express', 'extend', 'extens', 'extrem', 'facullti', 'faculti', 'faster', 'fbk', 'fellow', 'fellowship', 'final', 'free', 'fulli', 'fund', 'galwai', 'gener', 'gi', 'graduat', 'hire', 'human', 'icistm2012', 'icsoc', 'ict', 'iir', 'illinoi', 'imag', 'inaugur', 'indiana', 'industri', 'inform', 'institut', 'intellig', 'intens', 'interest', 'intern', 'invoc', 'issu', 'iswpc']\n",
      "NON-CFP\n",
      "['itali', 'itc', 'jnit', 'job', 'journal', 'juli', 'june', 'knowledg', 'lab', 'label', 'languag', 'lectur', 'level', 'link', 'louvain', 'maarten', 'manag', 'march', 'microsoft', 'mine', 'mobil', 'mobiwi', 'multimod', 'multipl', 'natur', 'ndt', 'netherland', 'network', 'nmr', 'nom', 'nov', 'novemb', 'npa', 'nui', 'oct', 'octob', 'onlin', 'ontolog', 'open', 'opportun', 'org', 'orient', 'padua', 'pakdd', 'panel', 'paper', 'pari', 'particip', 'pervas', 'phd', 'posit', 'post', 'postdoc', 'postdoctor', 'postion', 'privaci', 'proceed', 'process', 'professor', 'program', 'project', 'propos', 'protocol', 'proven', 'reason', 'remind', 'represent', 'research', 'retriev', 'risk', 'scienc', 'scientif', 'secur', 'semant', 'senior', 'sensor', 'servic', 'session', 'sigmod', 'smpe', 'soa', 'social', 'spain', 'spatial', 'special', 'steven', 'stream', 'studentship', 'submiss', 'symposuim', 'system', 'technic', 'technolog', 'tempor', 'tenur', 'tist', 'tomorrow', 'track', 'trento', 'trust', 'tue', 'tutori', 'ubiquit', 'unit', 'univers', 'usa', 'vacanc', 'valencia', 'vehicular', 'video', 'web', 'wireless', 'workflow', 'workshop', 'zurich']\n"
     ]
    }
   ],
   "source": [
    "def get_prototypes_and_labels(net, feature_names):\n",
    "    # the prototypes are the weights of the second layer\n",
    "    labeled_bags_of_words = []\n",
    "    prototypes = net.layers[1].np['w']\n",
    "    for prototype in prototypes:\n",
    "        bag_of_words = []\n",
    "        for index, entry in enumerate(prototype):\n",
    "            if entry == 1:\n",
    "                bag_of_words.append(feature_names[index])\n",
    "        label = 'CFP' if decode_labels(net.sim([prototype]))[0] == 1 else 'NON-CFP'\n",
    "        labeled_bags_of_words.append((label, bag_of_words))\n",
    "    return labeled_bags_of_words\n",
    "\n",
    "labeled_bags_of_words = get_prototypes_and_labels(net, feature_names)\n",
    "for label, bag_of_words in labeled_bags_of_words:\n",
    "    print label\n",
    "    print bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print net.sim([prototypes[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print len(prototypes[0])\n",
    "print prototypes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority baseline:  0.546875\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.545454545455\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:  0\n",
      "train range:  21 - 64\n",
      "test range:  0 - 21\n",
      "Epoch: 100; Error: 0.511627906977;\n",
      "Epoch: 200; Error: 0.0697674418605;\n",
      "The maximum number of train epochs is reached\n",
      "Accuracy:  0.809523809524\n",
      "k:  1\n",
      "train range:  0 - 21  |  42 - 64\n",
      "test range:  21 - 42\n",
      "Epoch: 100; Error: 0.395348837209;\n",
      "Epoch: 200; Error: 0.395348837209;\n",
      "The maximum number of train epochs is reached\n",
      "Accuracy:  0.428571428571\n",
      "k:  2\n",
      "train range:  0 - 42  |  63 - 64\n",
      "test range:  42 - 63\n",
      "Epoch: 100; Error: 0.441860465116;\n",
      "Epoch: 200; Error: 0.441860465116;\n",
      "The maximum number of train epochs is reached\n",
      "Accuracy:  0.52380952381\n"
     ]
    }
   ],
   "source": [
    "def crossfold(data, target, k, epochs):\n",
    "    \n",
    "    split = len(data) / k\n",
    "\n",
    "    for i in range(k):\n",
    "        test_start = i * split\n",
    "        test_end = (i+1) * split\n",
    "        train_start = test_end\n",
    "        train_end = len(data)\n",
    "\n",
    "        test_data = data[test_start:test_end]\n",
    "        train_data = data[train_start:train_end]\n",
    "        \n",
    "        test_target = target[test_start:test_end]\n",
    "        train_target = target[train_start:train_end]\n",
    "        \n",
    "        second_train_start = 0\n",
    "        second_train_end = 0\n",
    "        \n",
    "        print 'k: ', i\n",
    "\n",
    "        if i > 0:\n",
    "            second_train_end = test_start\n",
    "            train_data = np.vstack([train_data, data[second_train_start:second_train_end]])\n",
    "            train_target = np.vstack([train_target, target[second_train_start:second_train_end]])\n",
    "            print 'train range: ', second_train_start, '-', second_train_end, ' | ', train_start, '-' , train_end\n",
    "        else:\n",
    "            print 'train range: ', train_start, '-' , train_end\n",
    "            \n",
    "        print 'test range: ', test_start, '-', test_end\n",
    "            \n",
    "        net = nl.net.newlvq(nl.tool.minmax(train_data), 242, [.5, .5])\n",
    "        error = net.train(train_data, train_target, epochs=epochs, lr=0.1, goal=-1)\n",
    "        \n",
    "        output = net.sim(test_data)\n",
    "        evaluate(output, test_target)\n",
    "\n",
    "crossfold(data, target, 3, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Limited by technology choice, need for more evolved implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Kohonen, T.: 1986, Learning vector quantization for pattern recognition, Technical Report TKK-\n",
    "F-A601, Helsinki Univeristy of Technology, Espoo, Finland.\n",
    "\n",
    "Kohonen, T.: 1998, Learning vector quantization, The handbook of brain theory and neural net-\n",
    "works, MIT Press, Cambridge, MA, USA, pp. 537–540\n",
    "\n",
    "Martín-Valdivia, M. T., Ureña-López, L. A., & García-Vega, M.: 2007, The Learning Vector Quantization Algorithm Applied to\n",
    "Automatic Text Classification Tasks. Neural Netw., 20(6), 748–756. http://doi.org/10.1016/j.neunet.2006.12.005\n",
    "\n",
    "Neurolab. Retrieved July 8, 2016, from https://github.com/zueve/neurolab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
